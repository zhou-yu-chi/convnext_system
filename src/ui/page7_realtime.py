import os
import cv2
import torch
import torch.nn as nn
from torchvision import models, transforms
from PIL import Image

from PySide6.QtWidgets import (QWidget, QVBoxLayout, QHBoxLayout, QPushButton, 
                               QLabel, QComboBox, QMessageBox, QFrame, QApplication, QDoubleSpinBox)
from PySide6.QtCore import Qt, QThread, Signal, QTimer
from PySide6.QtGui import QImage, QPixmap

# æ²¿ç”¨ä½ åŸæœ¬çš„å®‰å…¨è¨­å‚™æª¢æ¸¬é‚è¼¯
def get_safe_device():
    if not torch.cuda.is_available(): return torch.device('cpu')
    try:
        major, minor = torch.cuda.get_device_capability(0)
        capability_score = major + minor / 10.0
        if capability_score < 3.7: return torch.device('cpu')
        test_conv = nn.Conv2d(1, 1, kernel_size=1).to('cuda')
        test_input = torch.randn(1, 1, 32, 32).to('cuda')
        _ = test_conv(test_input)
        return torch.device('cuda')
    except:
        return torch.device('cpu')

# ==========================================
# è² è²¬åœ¨èƒŒæ™¯ã€Œè®€å–ç›¸æ©Ÿ + åŸ·è¡Œæ¨è«–ã€çš„åŸ·è¡Œç·’
# ==========================================
class InferenceThread(QThread):
    update_frame = Signal(QImage)
    update_result = Signal(str, float, float) # æ¨™ç±¤, ä¿¡å¿ƒåº¦, æ¨è«–è€—æ™‚(ms)
    log_signal = Signal(str)

    def __init__(self, camera_index, model_path, strict_threshold):
        super().__init__()
        self.camera_index = camera_index
        self.model_path = model_path
        self.strict_threshold = strict_threshold
        self.is_running = True
        self.device = get_safe_device()
        self.cap = None

    def run(self):
        try:
            self.log_signal.emit(f"ğŸš€ æ­£åœ¨è¼‰å…¥æ¨¡å‹è‡³ {self.device}...")
            
            # --- 1. å‹•æ…‹é‡å»ºæ¨¡å‹æ¶æ§‹ ---
            model = models.convnext_tiny(weights=None)
            num_ftrs = model.classifier[2].in_features
            
            # è®€å– .pth æª”æ¡ˆ
            loaded_data = torch.load(self.model_path, map_location=self.device)
            
            # â˜…â˜…â˜… é—œéµè§£åŒ…é‚è¼¯ï¼šåˆ¤æ–·é€™æ˜¯ä¸æ˜¯ä¸€å€‹ã€Œæ‰“åŒ…éã€çš„å­—å…¸ â˜…â˜…â˜…
            if isinstance(loaded_data, dict) and "model_state_dict" in loaded_data:
                state_dict = loaded_data["model_state_dict"] # æå–çœŸæ­£çš„æ¬Šé‡
                self.log_signal.emit("â„¹ï¸ åµæ¸¬åˆ°æ‰“åŒ…æ¨¡å‹ï¼Œå·²æå– model_state_dict")
            else:
                state_dict = loaded_data # å¦‚æœæ˜¯ç´”æ¬Šé‡ï¼Œå°±ç…§èˆŠ
            
            # æª¢æŸ¥æ–°èˆŠç‰ˆçµæ§‹ (Dropout)
            if any("classifier.2.1" in k for k in state_dict.keys()):
                self.log_signal.emit("â„¹ï¸ åµæ¸¬åˆ°æ–°ç‰ˆæ¨¡å‹çµæ§‹ (å« Dropout)")
                model.classifier[2] = nn.Sequential(nn.Dropout(0.5), nn.Linear(num_ftrs, 2))
            else:
                self.log_signal.emit("â„¹ï¸ åµæ¸¬åˆ°èˆŠç‰ˆæ¨¡å‹çµæ§‹ (ä¸å« Dropout)")
                model.classifier[2] = nn.Linear(num_ftrs, 2)

            # è¼‰å…¥çœŸæ­£çš„æ¬Šé‡
            model.load_state_dict(state_dict, strict=False)
            model.to(self.device)
            model.eval()
            self.log_signal.emit("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼æ­£åœ¨é–‹å•Ÿç›¸æ©Ÿ...")

            # --- 2. å½±åƒå‰è™•ç† ---
            val_transforms = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
            classes = ['NG', 'OK']

            # --- 3. é–‹å•Ÿç›¸æ©Ÿ ---
            if os.name == 'nt':
                self.cap = cv2.VideoCapture(self.camera_index, cv2.CAP_DSHOW)
            else:
                self.cap = cv2.VideoCapture(self.camera_index)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            if not self.cap.isOpened():
                self.log_signal.emit("âŒ ç›¸æ©Ÿé–‹å•Ÿå¤±æ•—ï¼è«‹ç¢ºèªç›¸æ©Ÿæœªè¢«ä½”ç”¨ã€‚")
                return

            self.log_signal.emit("ğŸŸ¢ å³æ™‚æ¨è«–å·²å•Ÿå‹•ï¼")

            while self.is_running:
                ret, frame = self.cap.read()
                if not ret: continue

                start_time = cv2.getTickCount()
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # --- PyTorch æ¨è«– ---
                pil_img = Image.fromarray(rgb_frame)
                input_tensor = val_transforms(pil_img).unsqueeze(0).to(self.device)

                with torch.no_grad():
                    outputs = model(input_tensor)
                    probs = torch.nn.functional.softmax(outputs, dim=1)
                    _, preds = torch.max(outputs, 1)

                pred_idx = preds.item()
                confidence = probs[0][pred_idx].item()
                pred_label = classes[pred_idx]

                # åš´æ ¼æ¨¡å¼ï¼šOK ä¿¡å¿ƒä¸è¶³å¼·åˆ¶è½‰ NG
                if pred_label == 'OK' and confidence < self.strict_threshold:
                    pred_label = 'NG (åš´æ ¼æ¨¡å¼)'

                # è¨ˆç®—è€—æ™‚
                end_time = cv2.getTickCount()
                infer_time_ms = (end_time - start_time) / cv2.getTickFrequency() * 1000

                # ç™¼é€çµæœæ›´æ–° UI
                self.update_result.emit(pred_label, confidence, infer_time_ms)

                # ç™¼é€å½±åƒçµ¦ UI
                h, w, ch = rgb_frame.shape
                bytes_per_line = ch * w
                qt_image = QImage(rgb_frame.data, w, h, bytes_per_line, QImage.Format.Format_RGB888).copy()
                self.update_frame.emit(qt_image)

        except Exception as e:
            self.log_signal.emit(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        finally:
            if self.cap: self.cap.release()
            self.log_signal.emit("ğŸ”´ å³æ™‚æ¨è«–å·²åœæ­¢ã€‚")

    def stop(self):
        self.is_running = False
        self.wait(2000)

# ==========================================
# å³æ™‚æ¨è«–ä¸»é é¢ UI
# ==========================================
from PySide6.QtWidgets import QFileDialog

class Page7_RealtimeInference(QWidget):
    def __init__(self):
        super().__init__()
        self.inference_thread = None
        self.model_path = ""
        self.models_root = ""
        self.init_ui()

    def set_models_root(self, path):
        self.models_root = path

    def init_ui(self):
        layout = QVBoxLayout()
        layout.setContentsMargins(15, 15, 15, 15)
        layout.setSpacing(10)

        # --- 1. é ‚éƒ¨æ§åˆ¶åˆ— ---
        top_panel = QFrame()
        top_panel.setStyleSheet("background-color: #333; border-radius: 8px; padding: 10px;")
        top_layout = QHBoxLayout(top_panel)

        # è¼‰å…¥æ¨¡å‹æŒ‰éˆ•
        self.btn_load_model = QPushButton("ğŸ§  è¼‰å…¥æ¨¡å‹ (.pth)")
        self.btn_load_model.setStyleSheet("background-color: #ef6c00; color: white; font-weight: bold; padding: 8px 15px; border-radius: 5px;")
        self.btn_load_model.clicked.connect(self.on_load_model)
        
        self.lbl_model_name = QLabel("å°šæœªè¼‰å…¥æ¨¡å‹")
        self.lbl_model_name.setStyleSheet("color: #aaa; margin-right: 20px;")

        # ç›¸æ©Ÿé¸æ“‡
        lbl_cam = QLabel("ğŸ“· ç›¸æ©Ÿ:")
        self.combo_cam = QComboBox()
        self.combo_cam.addItem("é»æ“Šæƒæ...")
        self.combo_cam.setStyleSheet("QComboBox { background-color: #555; color: white; padding: 5px; border-radius: 4px; }")
        
        self.btn_refresh_cam = QPushButton("ğŸ”„ æƒæ")
        self.btn_refresh_cam.setStyleSheet("background-color: #555; color: white; padding: 5px 10px; border-radius: 5px;")
        self.btn_refresh_cam.clicked.connect(self.scan_available_cameras)

        # åš´æ ¼æ¨¡å¼é–€æª»
        lbl_strict = QLabel("ğŸ›¡ï¸ åš´æ ¼é–€æª»(OK):")
        self.spin_threshold = QDoubleSpinBox()
        self.spin_threshold.setRange(0.5, 0.99)
        self.spin_threshold.setSingleStep(0.05)
        self.spin_threshold.setValue(0.70)
        self.spin_threshold.setStyleSheet("QDoubleSpinBox { background-color: #555; color: white; padding: 5px; border-radius: 3px; }")

        # å•Ÿå‹•æ¨è«–æŒ‰éˆ•
        self.btn_toggle_infer = QPushButton("â–¶ï¸ é–‹å§‹å³æ™‚æ¨è«–")
        self.btn_toggle_infer.setStyleSheet("background-color: #388e3c; color: white; padding: 8px 20px; font-weight: bold; border-radius: 5px; font-size: 14px;")
        self.btn_toggle_infer.setCheckable(True)
        self.btn_toggle_infer.setEnabled(False) # æ²’è¼‰å…¥æ¨¡å‹å‰ä¸çµ¦æŒ‰
        self.btn_toggle_infer.clicked.connect(self.toggle_inference)

        top_layout.addWidget(self.btn_load_model)
        top_layout.addWidget(self.lbl_model_name)
        top_layout.addWidget(lbl_cam)
        top_layout.addWidget(self.combo_cam)
        top_layout.addWidget(self.btn_refresh_cam)
        top_layout.addSpacing(20)
        top_layout.addWidget(lbl_strict)
        top_layout.addWidget(self.spin_threshold)
        top_layout.addStretch()
        top_layout.addWidget(self.btn_toggle_infer)

        layout.addWidget(top_panel)

        # --- 2. ä¸­é–“ç•«é¢å€ ---
        middle_layout = QHBoxLayout()
        
        # å½±åƒé¡¯ç¤ºå€
        self.lbl_video = QLabel("ç­‰å¾…å•Ÿå‹•ç›¸æ©Ÿ...")
        self.lbl_video.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.lbl_video.setStyleSheet("background-color: #1a1a1a; border: 2px dashed #555; border-radius: 8px; font-size: 18px; color: #888;")
        self.lbl_video.setMinimumSize(640, 480)
        
        # å³å´ç‹€æ…‹å€
        status_panel = QFrame()
        status_panel.setFixedWidth(250)
        status_panel.setStyleSheet("background-color: #2b2b2b; border-radius: 8px; border: 1px solid #444;")
        status_layout = QVBoxLayout(status_panel)

        lbl_status_title = QLabel("å³æ™‚åˆ¤å®šçµæœ")
        lbl_status_title.setAlignment(Qt.AlignmentFlag.AlignCenter)
        lbl_status_title.setStyleSheet("font-size: 18px; font-weight: bold; color: #aaa; border: none;")

        self.lbl_result = QLabel("--")
        self.lbl_result.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.lbl_result.setStyleSheet("font-size: 60px; font-weight: bold; color: #555; border: none; margin: 20px 0px;")

        self.lbl_conf = QLabel("ä¿¡å¿ƒåº¦: --%")
        self.lbl_conf.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.lbl_conf.setStyleSheet("font-size: 16px; color: #cfcfcf; border: none;")

        self.lbl_fps = QLabel("æ¨è«–å»¶é²: -- ms")
        self.lbl_fps.setAlignment(Qt.AlignmentFlag.AlignCenter)
        self.lbl_fps.setStyleSheet("font-size: 14px; color: #888; border: none; margin-top: 10px;")

        status_layout.addWidget(lbl_status_title)
        status_layout.addWidget(self.lbl_result)
        status_layout.addWidget(self.lbl_conf)
        status_layout.addWidget(self.lbl_fps)
        status_layout.addStretch()

        middle_layout.addWidget(self.lbl_video, 1)
        middle_layout.addWidget(status_panel)
        
        layout.addLayout(middle_layout, 1)
        self.setLayout(layout)

        # å•Ÿå‹•æ™‚æƒæç›¸æ©Ÿ
        QTimer.singleShot(100, self.scan_available_cameras)

    def on_load_model(self):
        start_path = self.models_root if self.models_root else ""
        
        path, _ = QFileDialog.getOpenFileName(self, "é¸æ“‡è¦éƒ¨ç½²çš„æ¨¡å‹", start_path, "PyTorch Model (*.pth)")
        
        if path:
            self.model_path = path
            self.lbl_model_name.setText(os.path.basename(path))
            self.btn_toggle_infer.setEnabled(True)
            self.lbl_model_name.setStyleSheet("color: #4db6ac; font-weight: bold; margin-right: 20px;")

    def scan_available_cameras(self):
        self.combo_cam.clear()
        self.combo_cam.addItem("æƒæä¸­...")
        QApplication.processEvents()

        available_cams = []
        for i in range(4):
            # â˜…â˜…â˜… ä¿®æ­£ï¼šæƒæä¹Ÿè¦åŠ  CAP_DSHOW
            cap = cv2.VideoCapture(i, cv2.CAP_DSHOW) if os.name == 'nt' else cv2.VideoCapture(i)
            if cap is not None and cap.isOpened():
                available_cams.append(i)
                cap.release()

        self.combo_cam.clear()
        if not available_cams:
            self.combo_cam.addItem("âŒ æ‰¾ä¸åˆ°ç›¸æ©Ÿ")
        else:
            for cam_id in available_cams:
                self.combo_cam.addItem(f"ç›¸æ©Ÿ {cam_id}", cam_id)

    def toggle_inference(self):
        is_pressed = self.btn_toggle_infer.isChecked()
        
        if is_pressed:
            if self.combo_cam.count() == 0 or "æ‰¾" in self.combo_cam.currentText():
                QMessageBox.warning(self, "è­¦å‘Š", "è«‹å…ˆç¢ºèªç›¸æ©Ÿå·²é€£æ¥ï¼")
                self.btn_toggle_infer.setChecked(False)
                return

            self.btn_toggle_infer.setText("â¹ï¸ åœæ­¢æ¨è«–")
            self.btn_toggle_infer.setStyleSheet("background-color: #d32f2f; color: white; padding: 8px 20px; font-weight: bold; border-radius: 5px; font-size: 14px;")
            self.btn_load_model.setEnabled(False)
            self.spin_threshold.setEnabled(False)
            
            cam_idx = self.combo_cam.currentData()
            thresh = self.spin_threshold.value()

            self.inference_thread = InferenceThread(cam_idx, self.model_path, thresh)
            self.inference_thread.update_frame.connect(self.update_video)
            self.inference_thread.update_result.connect(self.update_ui_result)
            
            # â˜…â˜…â˜… ä¿®æ­£ï¼šæ¥ä¸Š log_signalï¼Œä¸ç„¶å‡ºéŒ¯äº†æœƒå®Œå…¨æ²’ç•«é¢ä¹Ÿæ²’å ±éŒ¯ï¼
            self.inference_thread.log_signal.connect(lambda msg: print(f"[Page7 Log] {msg}"))
            
            self.inference_thread.start()
        else:
            self.stop_inference()

    def stop_inference(self):
        # â˜…â˜…â˜… ä¿®æ­£ 4ï¼šå®Œæ•´é‡‹æ”¾åŸ·è¡Œç·’èˆ‡å¼·åˆ¶æŒ‰éˆ•ç‹€æ…‹å›æ­¸ â˜…â˜…â˜…
        if self.inference_thread:
            self.inference_thread.stop()
            self.inference_thread = None # å¾¹åº•æ¸…ç©ºï¼Œé¿å…æ®˜ç•™
            
        self.btn_toggle_infer.setChecked(False) # å¼·åˆ¶å°‡æŒ‰éˆ•è¨­ç‚ºã€Œæœªé»æ“Šã€ç‹€æ…‹
        self.btn_toggle_infer.setText("â–¶ï¸ é–‹å§‹å³æ™‚æ¨è«–")
        self.btn_toggle_infer.setStyleSheet("background-color: #388e3c; color: white; padding: 8px 20px; font-weight: bold; border-radius: 5px; font-size: 14px;")
        
        self.btn_load_model.setEnabled(True)
        self.spin_threshold.setEnabled(True)
        
        self.lbl_video.clear()
        self.lbl_video.setText("æ¨è«–å·²æš«åœ")
        self.lbl_result.setText("--")
        self.lbl_result.setStyleSheet("font-size: 60px; font-weight: bold; color: #555; border: none; margin: 20px 0px;")
        self.lbl_conf.setText("ä¿¡å¿ƒåº¦: --%")
        self.lbl_fps.setText("æ¨è«–å»¶é²: -- ms")

    def update_video(self, qt_image):
        pixmap = QPixmap.fromImage(qt_image)
        scaled = pixmap.scaled(self.lbl_video.width(), self.lbl_video.height(), 
                               Qt.AspectRatioMode.KeepAspectRatio, Qt.TransformationMode.SmoothTransformation)
        self.lbl_video.setPixmap(scaled)

    def update_ui_result(self, label, conf, infer_time):
        self.lbl_result.setText(label)
        self.lbl_conf.setText(f"ä¿¡å¿ƒåº¦: {conf:.1%}")
        self.lbl_fps.setText(f"æ¨è«–å»¶é²: {infer_time:.1f} ms")

        # æ”¹è®Šé¡è‰²æç¤º
        if "NG" in label:
            self.lbl_result.setStyleSheet("font-size: 50px; font-weight: bold; color: #e57373; border: none; margin: 20px 0px;")
        else:
            self.lbl_result.setStyleSheet("font-size: 60px; font-weight: bold; color: #81c784; border: none; margin: 20px 0px;")

    def hideEvent(self, event):
        """åˆ‡æ›åˆ°å…¶ä»–é é¢æ™‚ï¼Œè‡ªå‹•é—œé–‰æ¨è«–ï¼Œé‡‹æ”¾ç›¸æ©Ÿèˆ‡ GPU è³‡æº"""
        if self.btn_toggle_infer.isChecked():
            self.btn_toggle_infer.setChecked(False)
            self.stop_inference()
        super().hideEvent(event)